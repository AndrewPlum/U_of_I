{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e371522-f3ee-4da9-a9f6-97fb3a5edaaa",
   "metadata": {},
   "source": [
    "## Lecture 11: Implement a fully connected NN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7508017f-7afa-4cbd-ab71-30663df5f64a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d355bc-0ff2-4741-be9b-36b9636819ef",
   "metadata": {},
   "source": [
    "#### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e4c43626-e84f-4d47-9002-5b78f7285730",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: X: (4898, 11) Y: (4898,)\n",
      "(3428, 11) (1470, 11)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('winequality-white.csv', sep = ';')\n",
    "df\n",
    "X = df.values[:, :11]\n",
    "Y = df.values[:, 11]\n",
    "print('Data shape:', 'X:', X.shape, 'Y:', Y.shape)\n",
    "\n",
    "# data normalization\n",
    "min_vals = np.min(X, axis = 0)\n",
    "max_vals = np.max(X, axis = 0)\n",
    "X = (X-min_vals)/(max_vals-min_vals) \n",
    "\n",
    "#\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Y, test_size = 0.3, random_state = 0)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10ca25-eaee-4759-9e84-30258b12bee4",
   "metadata": {},
   "source": [
    "#### 2. Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "68476b8b-72c7-4669-8502-64e8e2c6670b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73105858, 0.88079708, 0.95257413],\n",
       "       [0.98201379, 0.88079708, 0.5       ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigm(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def dsigm(z):\n",
    "    return sigm(z)*(1 - sigm(z)) \n",
    "\n",
    "z = np.array([[1, 2, 3], [4, 2, 0]])\n",
    "sigm(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfb1d3-6813-406a-b304-c25c6f4f1755",
   "metadata": {},
   "source": [
    "#### 3. Create NN layers using Python OOP\n",
    "1) An NN layer contains\n",
    "\n",
    "    - parameters: **a number of nodes/units, weight matrix, and bias**\n",
    "    - operations: **net input calculation** and **activation function**\n",
    "\n",
    "2) Create a layer class that could generate layers with any number of nodes and multiple activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "51c6897d-0521-4a83-b6b6-7cb3da5f39ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
      "   0.95008842 -0.15135721 -0.10321885  0.4105985   0.14404357]\n",
      " [ 1.45427351  0.76103773  0.12167502  0.44386323  0.33367433  1.49407907\n",
      "  -0.20515826  0.3130677  -0.85409574 -2.55298982  0.6536186 ]\n",
      " [ 0.8644362  -0.74216502  2.26975462 -1.45436567  0.04575852 -0.18718385\n",
      "   1.53277921  1.46935877  0.15494743  0.37816252 -0.88778575]\n",
      " [-1.98079647 -0.34791215  0.15634897  1.23029068  1.20237985 -0.38732682\n",
      "  -0.30230275 -1.04855297 -1.42001794 -1.70627019  1.9507754 ]\n",
      " [-0.50965218 -0.4380743  -1.25279536  0.77749036 -1.61389785 -0.21274028\n",
      "  -0.89546656  0.3869025  -0.51080514 -1.18063218 -0.02818223]] \n",
      "bias: [[ 0.42833187]\n",
      " [ 0.06651722]\n",
      " [ 0.3024719 ]\n",
      " [-0.63432209]\n",
      " [-0.36274117]]\n",
      "L_1 output: [[0.92724619 0.6413901  0.8391263  0.24196598 0.07592622]]\n",
      "L_out output: [[2.42039114]]\n"
     ]
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, units, input_dim, activation = None):#initialize layer parameters\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.input_dim = input_dim # nodes of previous layer\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        self.W = np.random.randn(self.units, self.input_dim)\n",
    "        self.bias = np.random.randn(self.units, 1)\n",
    "        \n",
    "    def run(self, inputs):# layer operations: net input + activation fundtion\n",
    "        ''' calculate the net input and activation output of the current layer  \n",
    "            inputs=(n_sample * n_features)    \n",
    "            return the activation output\n",
    "        '''\n",
    "        #calculate the net input\n",
    "        self.net = np.dot(inputs, self.W.T) + self.bias.T\n",
    "       \n",
    "        #activation output \n",
    "        if self.activation == 'sigm':\n",
    "            self.output = sigm(self.net)\n",
    "        if self.activation == None: #linear layer\n",
    "            self.output = self.net \n",
    "            \n",
    "        return self.output\n",
    "    \n",
    "## create Layer object    \n",
    "L1 = Layer(units = 5, input_dim = 11, activation = 'sigm')\n",
    "L_out = Layer(units = 1, input_dim = 5)\n",
    "print('W:', L1.W, '\\nbias:', L1.bias)\n",
    "\n",
    "inputs = X_train[0:1]\n",
    "h = L1.run(inputs)\n",
    "print('L_1 output:', h)\n",
    "print('L_out output:', L_out.run(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132466df-ed09-42d7-b64e-4f5c7c952939",
   "metadata": {},
   "source": [
    "#### 4. Create an actual NN\n",
    "An NN contains \n",
    "\n",
    "    - multiple layers: input layer, hidden layer(s), and output layer\n",
    "    - add any number of layers\n",
    "    - forward propagation\n",
    "    - loss function\n",
    "    - training function(optimization) that uses SGD and BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a30450a-84e7-44e6-89bf-482f79c93b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "nn layers: [<__main__.Layer object at 0x000001DD5BDCA950>, <__main__.Layer object at 0x000001DD5BDE81D0>]\n",
      "output layer weights: [[1.76405235 0.40015721 0.97873798 2.2408932  1.86755799]]\n",
      "[[2.42039114]]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers=[] # list of layers\n",
    "        \n",
    "    # implement the 'add' function     \n",
    "    def add(self, units, input_dim, activation = 'sigm'):\n",
    "        '''add one layer to neural network\n",
    "            parameters:\n",
    "                units: the number of nodes of current layer\n",
    "                input_dim: input dimension (the number of nodes of the previous layer)\n",
    "                activation: the activation function\n",
    "        '''\n",
    "        \n",
    "        ## add your code here\n",
    "        L = Layer(units, input_dim, activation)\n",
    "        self.layers.append(L)\n",
    "    \n",
    "    def forward_prop(self, inputs):\n",
    "        '''forward propagation calculates net input and output for each layer \n",
    "            inputs: input data(n_samples * n_features)\n",
    "            return the output of the last layer          \n",
    "        '''\n",
    "        \n",
    "        nLayers = len(self.layers)\n",
    "        for i in range(nLayers):\n",
    "            out = self.layers[i].run(inputs)\n",
    "            inputs = out   \n",
    "        return out\n",
    "     \n",
    "    def forward_prop(self, inputs):\n",
    "        '''forward propagation calculates net input and output for each layer\n",
    "        \n",
    "            inputs: input data(n_samples * n_features)\n",
    "            return the output of the last layer\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        nLayers = len(self.layers)\n",
    "        #print(nLayers)\n",
    "        for i in range(nLayers):\n",
    "            out = self.layers[i].run(inputs)\n",
    "            inputs = out   \n",
    "        return out\n",
    "    \n",
    "    def loss(self, y_pred, y):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def train(self, inputs, targets, lr = 0.001, batch_size = 32, epochs = 50):\n",
    "        '''implement the SGD process and use Back-Propagation algorithm to calculate gradients \n",
    "        \n",
    "            inputs: training samples\n",
    "            targets: training targets\n",
    "            lr: learning rate\n",
    "            batch_size: batch size\n",
    "            epochs: max number of epochs\n",
    "        '''\n",
    "        pass\n",
    "   \n",
    "    # Task 3.6: implement the BP algorithm. 30 points\n",
    "    def BP(self, x, y):\n",
    "        ''' Back-propagation algorithm\n",
    "        \n",
    "            x: input samples (n_samples * n_features)\n",
    "            y: ont-hot targets (n_samples * 10)\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "myNN = NeuralNetwork()\n",
    "print(myNN.layers)\n",
    "\n",
    "myNN.add(units = 5, input_dim= 11, activation = 'sigm')#5 nodes in hidden layer\n",
    "myNN.add(units = 1, input_dim= 5, activation = None) #1 node in output layer\n",
    "\n",
    "print('nn layers:', myNN.layers)\n",
    "print('output layer weights:', myNN.layers[1].W)\n",
    "\n",
    "y_hat = myNN.forward_prop(X_train[0:1])\n",
    "print('prediction:', y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2ab70-bfbc-49a0-acf0-e93eb6a070f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3: Implement NN from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: X: (4898, 11) Y: (4898,)\n",
      "(3428, 11) (1470, 11)\n",
      "(3428, 1) (1470, 1)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('winequality-white.csv', sep = ';')\n",
    "X = df.values[:, :11]\n",
    "Y = df.values[:, 11]\n",
    "print('Data shape:', 'X:', X.shape, 'Y:', Y.shape)\n",
    "\n",
    "# data normalization\n",
    "min_vals = np.min(X, axis = 0)\n",
    "max_vals = np.max(X, axis = 0)\n",
    "X = (X-min_vals)/(max_vals-min_vals) \n",
    "\n",
    "# train set and test set preparation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
    "y_train =  np.expand_dims(y_train, axis=1)\n",
    "y_test =  np.expand_dims(y_test, axis=1)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Activation functions. 5 points\n",
    "\n",
    "1.1 Implement the ReLU function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax output:\n",
      " [[0.04712342 0.94649912 0.00637746]\n",
      " [0.49938108 0.00123784 0.49938108]]\n",
      "ReLU output:\n",
      " [[0 2 0]\n",
      " [4 0 4]]\n"
     ]
    }
   ],
   "source": [
    "def sigm(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def dsigm(z):\n",
    "    return sigm(z)*(1 - sigm(z))\n",
    "\n",
    "def softmax(z):\n",
    "    '''   \n",
    "        parameter: z: m_batch * n_features\n",
    "        return: m_batch * n_features\n",
    "            \n",
    "    '''\n",
    "    b = np.exp(z - np.max(z, axis = 1, keepdims = True))# np.max(z)\n",
    "    return b / np.sum(b, axis = 1, keepdims = True)\n",
    "\n",
    "\n",
    "def ReLU(z):\n",
    "    ''' The ReLU function\n",
    "    \n",
    "        parameter:\n",
    "            z: m_batch * n_features\n",
    "        return: m_batch * n_features\n",
    "            \n",
    "    '''\n",
    "    #-----add your code here----\n",
    "\n",
    "    \n",
    "    #-------------------------\n",
    "    \n",
    "z = np.array([[-1, 2, -3], [4, -2, 4]])\n",
    "print('softmax output:\\n', softmax(z))\n",
    "print('ReLU output:\\n', ReLU(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Create the lalyer class. 25 points\n",
    "\n",
    "    2.1 Implement the Xavier initialization to initilize W and bias. 10 points\n",
    "    2.2 Complete the run function. 5 points\n",
    "    2.3 Create layers and process data samples. 10 poits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "   \n",
    "    def __init__(self, units, input_dim, activation = None): \n",
    "        #Student can use any method. But cannot initialize W and bias as zeros.\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        \n",
    "        # 2.1 Implement the Xavier initialization to initilize W and bias \n",
    "        #check here: https://www.deeplearning.ai/ai-notes/initialization/\n",
    "        ## add you code here-------------\n",
    " \n",
    "\n",
    "\n",
    "        ##----------------------\n",
    "        \n",
    "        self.gW = np.zeros((units, input_dim))\n",
    "        self.gb = np.zeros((units,1))\n",
    "        \n",
    "    def run(self, inputs):\n",
    "        ''' calculate the net input and activation output of the current layer\n",
    "        \n",
    "            inputs=(n_sample * n_features)\n",
    "          \n",
    "            return the activation output\n",
    "        '''\n",
    "        #calculate the net input\n",
    "        \n",
    "        self.net = np.dot(inputs, self.W.T) + self.bias.T\n",
    "       \n",
    "        #2.2 activation output \n",
    "        ## add you code here--------\n",
    "\n",
    "        \n",
    "        \n",
    "        ##------------------------------     \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [[ 0.53188179  0.12065194  0.29510061  0.67565472  0.56308992 -0.29466037\n",
      "   0.28646244 -0.04563592 -0.03112165  0.12380011  0.04343077]] Bias: [[1.45427351]]\n",
      "Output: [[2.09251453]\n",
      " [1.94740246]]\n",
      "weights: [[ 0.53188179  0.12065194  0.29510061  0.67565472  0.56308992 -0.29466037\n",
      "   0.28646244 -0.04563592 -0.03112165  0.12380011  0.04343077]] Bias: [[1.45427351]]\n",
      "Output: [[0.8901735 ]\n",
      " [0.87516313]]\n"
     ]
    }
   ],
   "source": [
    "## 2.3 create a liner and non-linear layers with 2 nodes/units, input the first two training samples, and print the output\n",
    "\n",
    "#add your code here----------------------\n",
    "# create a linear layer with 2 nodes/units and print out for processing the first two samples\n",
    "\n",
    "\n",
    "\n",
    "# create a non-linear layers with 2 nodes/units and the sigmoid activation and print out for processing the first two samples\n",
    "\n",
    "\n",
    "#----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Create NN. 50 points\n",
    "\n",
    "    3.1 Implement the loss function. 5 points\n",
    "    3.2 Complete the following 'train' function. 10 points\n",
    "    3.3 Implement the BP algorithm. CS 474 students will earn 10 extra points for processing more than three layers. It is required for CS574 students to implement BP for processing more than 3 layers. 30 points\n",
    "    3.4 update all weights and bias. 5 points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers=[] #list of layers\n",
    "        \n",
    "    def add(self, units, input_dim, activation = None):\n",
    "        '''add layers to NN\n",
    "            \n",
    "            units: the number of nodes\n",
    "            input_dims: input dimensions\n",
    "            activation: activation fundtions\n",
    "        \n",
    "        '''\n",
    "        layer = Layer(units, input_dim, activation)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward_prop(self, inputs):\n",
    "        '''forward propagation calculates net input and output for each layer\n",
    "        \n",
    "            inputs: input data(n_samples * n_features)\n",
    "            return the output of the last layer: n_samples * n_output_nodes\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        nLayers = len(self.layers)\n",
    "        #print(nLayers)\n",
    "        for i in range(nLayers):\n",
    "            out = self.layers[i].run(inputs)\n",
    "            inputs = out   \n",
    "        return out\n",
    "    \n",
    "    # 3.1 implement the loss function. 5 points\n",
    "    def loss(self, y_pred, y):\n",
    "        '''loss function: 1/(2*n_samples)*sum_samples((y_i-y_pred_i)**2)\n",
    "            y_pred: predictions(n_samples * 1)\n",
    "            y: target(n_samples * 1)\n",
    "        '''\n",
    "        #add your code here-------------------\n",
    "\n",
    "        \n",
    "        #------------------------------------\n",
    "    \n",
    "    # implement the prediction function.\n",
    "    def predict(self, x):\n",
    "        '''predict\n",
    "            x: input(n_samples * n_features) \n",
    "            return predicted values\n",
    "        '''\n",
    "        out = self.forward_prop(x)\n",
    "        return out\n",
    "         \n",
    "\n",
    "    #3.2 complete the following 'train' function. 10 points\n",
    "    def train(self, inputs, targets, lr = 0.001, batch_size = 32, epochs = 50):\n",
    "        '''implement the SGD process and use Back-Propagation algorithm to calculate gradients \n",
    "        \n",
    "            inputs: training samples\n",
    "            targets: training targets\n",
    "            lr: learning rate\n",
    "            batch_size: batch size\n",
    "            epochs: max number of epochs\n",
    "        '''\n",
    "        \n",
    "        m = len(targets)  \n",
    "        loss_hist = np.zeros(epochs)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            #shuffle the data\n",
    "            idx = np.arange(m)\n",
    "            np.random.shuffle(idx)\n",
    "            inputs = inputs[idx]\n",
    "            targets = targets[idx]\n",
    "            \n",
    "            for b in range(int(m/batch_size)):\n",
    "                b_start= b*batch_size\n",
    "                b_end = min((b+1)*batch_size, m)\n",
    "                \n",
    "                x_batch = inputs[b_start:b_end, :]\n",
    "                y_batch = targets[b_start:b_end, :]\n",
    "                #print(x_batch.shape)\n",
    "                \n",
    "                #add your code here------------------------------\n",
    "                # 1)run forward propagation to get output of the current mini-batch\n",
    "\n",
    "                \n",
    "                # 2)call BP to calculate all gradients\n",
    "\n",
    "                \n",
    "                # 3)update all weights and bias\n",
    "\n",
    "                #-------------------------------------------------\n",
    "                \n",
    "            lr = lr*0.99  \n",
    "            #add your code here---------------------------\n",
    "            # 4) calculate and record the loss of current epoch\n",
    "\n",
    "            # 5) print out the loss of current epoch\n",
    "\n",
    "            #-----------------------------------------\n",
    "        return loss_hist\n",
    "   \n",
    "    #3.3 implement the BP algorithm. 30 points\n",
    "    def BP(self, x, y):\n",
    "        ''' Back-propagation algorithm\n",
    "        \n",
    "            x: input samples (n_samples * n_features)\n",
    "            y: predicted values\n",
    "        '''\n",
    "        \n",
    "        nLayers = len(self.layers)\n",
    "        m_batch = x.shape[0]\n",
    "        \n",
    "        #add your code here------------------------------------\n",
    "        # CS474 students are required to perform BP for three layers\n",
    "        # CS 574 students are required to perform BP for more than three layers\n",
    "\n",
    "\n",
    "        #------------------------------------------------------------\n",
    "        \n",
    "    #3.4 update all weights and bias. 5 points         \n",
    "    def updateWeights(self, lr):\n",
    "        nLayers = len(self.layers)\n",
    "        #add your code here------------------------------------------------------\n",
    "\n",
    "        #------------------------------------------------------------------------       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Evaluation. 20 points\n",
    "\n",
    "    4.1 Create a three-layer NN: The hidden layer has 10 units with sigmoid activation, and the output layer has one unit with None activation.\n",
    "    4.2 Train the network by calling the train function using the training set: lr = 0.001, batch_size = 8, epochs = 50\n",
    "    4.3 Plot the loss_hist from the training results\n",
    "    4.4 Evaluate the trained nn by calcualating the MSE and MAE using using both the training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 and 4.2 create and train the nn. 5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#add your code here--\n",
    "\n",
    "#------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Plot the loss_hist. 5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#add your code here--\n",
    "\n",
    "#------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 Evaluate the trained nn by calcualating the MSE and MAE using using both the training and test sets. 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#add your code here--\n",
    "\n",
    "#------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5(Optional): Students get 10 extra points for getting better test performance using \n",
    "    1)different hyperparameters(batch size, lr, and epochs) and/or \n",
    "    2)nn design, e.g., different number of hidden nodes, different number of hidden layers, different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#add your code here--\n",
    "\n",
    "#------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

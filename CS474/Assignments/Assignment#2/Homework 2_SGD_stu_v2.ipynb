{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2: SGD(minibatch) Method for Wine Quality Prediction \n",
    "\n",
    "Add your code to the following sections:\n",
    "\n",
    "    ## add your code here\n",
    "    #--------------------------------\n",
    "\n",
    "    #---------------------------------\n",
    "    \n",
    "Description: In this homework, we will practice cross-validation and implement the stochastic gradient optimization (mini-batch) to solve the wine quality prediction problem. Using the following code as your template. Specific requirements:\n",
    "\n",
    "1. Use all function definitions given in the code (e.g., def SGD(X, Y, lr = 0.001, batch_size = 32, epoch = 100):); and do not change the function names and input arguments.\n",
    "\n",
    "2. Evaluate (Cross-validation) the model trained using GD (20 points)\n",
    "\n",
    "3. SGD implementation. 40 pts\n",
    "   \n",
    "4. Calculate and print out the MSE and MAE values of SGD for the training and test sets (15 points)\n",
    "5. Plot the loss curve of the SGD. (5 points)\n",
    "6. Plot the mse curves on the training and test sets using different models (w_hist). (20 points)\n",
    "\n",
    "### Common mistakes\n",
    "    \n",
    "1. Call GD and SGD using the whole dataset\n",
    "\n",
    "    -- GD and SGD are used to train the model (learn w); and we should call them using the training sets\n",
    "   \n",
    "2. Calculate gradient using the whole training set for SGD\n",
    "    \n",
    "    -- In SGD, update gradient only using mini-batches\n",
    "  \n",
    "3. Calculate the loss of each epoch using the average of all minibatches\n",
    "    \n",
    "    -- should use the w of the last mini-batch and the whole training set to calculate the loss  \n",
    "   \n",
    "4. Mix concepts of loss function and evaulation metrics\n",
    "    -- loss function: for optimization purpose (gradient). We use the sum of square errors in this homework. L = 1/2 * sum(y_hat_i - y_i)^2\n",
    "    \n",
    "    -- evaluation metrics: mse and mae: mse = 1/m * sum(y_hat_i - y_i)^2, mae = 1/m * sum(abs(y_hat_i - y_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data, implement the model, loss function and GD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: X: (4898, 11) Y: (4898,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## (1) Data preparation\n",
    "df=pd.read_csv('winequality-white.csv', sep = ';')\n",
    "df\n",
    "X = df.values[:, :11] #input feature vectors\n",
    "Y = df.values[:, 11] # target values\n",
    "print('Data shape:', 'X:', X.shape, 'Y:', Y.shape)\n",
    "\n",
    "# data normalization. Please use X1 rather than X in the rest of this homework\n",
    "min_vals = np.min(X, axis = 0)\n",
    "max_vals = np.max(X, axis = 0)\n",
    "X1 = (X-min_vals)/(max_vals-min_vals)\n",
    "\n",
    "##(2) Assume a linear mode that y = w0*1 + w_1*x_1 +w_2*x_2+...+ w_11*x_11\n",
    "def predict(X, w):\n",
    "    '''\n",
    "    X: input feature vectors:n_samples * n_features\n",
    "    w: weights\n",
    "    \n",
    "    return Y_hat\n",
    "    '''\n",
    "    # \n",
    "    Y_hat = np.zeros((X.shape[0]))\n",
    "    for idx, x in enumerate(X):          \n",
    "        y_hat = w[0] + np.dot(w[1:].T, np.c_[x]) # linear model\n",
    "        Y_hat[idx] = y_hat    \n",
    "    return Y_hat\n",
    "\n",
    "## (3) Loss function: L = 1/2 * sum(y_hat_i - y_i)^2\n",
    "def loss(w, X, Y):\n",
    "    '''\n",
    "    w: weights\n",
    "    X: input feature vectors\n",
    "    Y: targets\n",
    "    '''\n",
    "    Y_hat = predict(X, w)\n",
    "    loss = 1/2* np.sum(np.square(Y - Y_hat))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Optimization Algo 1: Gradient Descent\n",
    "def GD(X, Y, lr = 0.001, delta = 0.01, max_iter = 100):\n",
    "    '''\n",
    "    X: training data\n",
    "    Y: training target\n",
    "    lr: learning rate\n",
    "    max_iter: the max iterations\n",
    "    '''\n",
    "    \n",
    "    m = len(Y)\n",
    "    b = np.reshape(Y, [Y.shape[0],1])\n",
    "    w = np.random.rand(X.shape[1] + 1, 1) #w^0\n",
    "    A = np.c_[np.ones((m, 1)), X]\n",
    "    gradient = A.T.dot(np.dot(A, w)-b)\n",
    "    \n",
    "    loss_hist = np.zeros(max_iter) # history of loss\n",
    "    w_hist = np.zeros((max_iter, w.shape[0])) # history of weight\n",
    "    loss_w = 0\n",
    "    i = 0                  \n",
    "    while(np.linalg.norm(gradient) > delta) and (i < max_iter):\n",
    "        w_hist[i,:] = w.T\n",
    "        loss_w = loss(w, X, Y)\n",
    "        print(i, 'loss:', loss_w)\n",
    "        loss_hist[i] = loss_w\n",
    "        \n",
    "        w = w - lr*gradient        \n",
    "        gradient = A.T.dot(np.dot(A, w)-b) # update the gradient using new w\n",
    "        i = i + 1\n",
    "        \n",
    "    w_star = w  \n",
    "    return w_star, loss_hist, w_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model evaluation using cross-validation (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Split the dataset (X1, Y) into a training (70%) set, (X_train, y_train), and a test (30%) set, (X_test, y_test). (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "#print(X_train.shape, X_test.shape)\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model training using the GD function with different learning rate (10 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## add your code here\n",
    "#-----------------------\n",
    "lr = 0.0001\n",
    "\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add your code here\n",
    "#-----------------------\n",
    "lr = 0.0001\n",
    "\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What do you observed from the results of two learning rates?\n",
    "Add your response here:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Evaluate the trained model (w_star) using the MSE and MAE on the training set and test set, respectively. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training error: MSE, MAE\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "\n",
    "#print('training mse: {} and training mae:{}'.format(mse_train, mae_train))\n",
    "#---------------------------------\n",
    "\n",
    "\n",
    "## test error: MAE, MSE\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "\n",
    "#print('test mse: {} and test mae:{}'.format(mse_test, mae_test))\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SGD implementation (35 points)\n",
    "Use the SGD function definition given in the code (def SGD(X, Y, lr = 0.001, batch_size = 32, epoch = 100):); and do not change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 26 (138578501.py, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 44\u001b[1;36m\u001b[0m\n\u001b[1;33m    lr = lr * 0.9\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 26\n"
     ]
    }
   ],
   "source": [
    "def SGD(X, Y, lr = 0.001, batch_size = 32, epoch = 100): \n",
    "    '''Implement the minibatch Gradient Desent approach\n",
    "    \n",
    "        X: training data\n",
    "        Y: training target\n",
    "        lr: learning rate\n",
    "        batch_size: batch size\n",
    "        epoch: number of max epoches\n",
    "        \n",
    "        return: w_star, w_hist, loss_hist\n",
    "    '''\n",
    "    m = len(Y)\n",
    "    np.random.seed(9)\n",
    "    w = np.random.rand(X.shape[1]+1, 1)    #(12,1) values in [0, 1)\n",
    "    w_hist = np.zeros((epoch, w.shape[0])) # (epoch,12) \n",
    "    loss_hist = np.zeros(epoch)            # (epoch,)\n",
    "   \n",
    "    \n",
    "    ## add your code here\n",
    "    #-----------------------\n",
    "    for i in range(epoch):\n",
    "        #(1) Shuffle data (X and Y) at the beginning of each epoch. (5 points)\n",
    "\n",
    "        \n",
    "        #(2) go through all minibatches and update w. (25 points)\n",
    "        for b in range(int(m/batch_size)): \n",
    "            #prepare the bth minibatch X_batch and Y_batch. \n",
    "\n",
    "            \n",
    "            #prepare A and b for current minibatch. \n",
    "\n",
    "            \n",
    "            #gradient calcualation and w update. \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        ## (3) Save the loss(on whole training set) and the weight for current epoch. 5 points\n",
    "\n",
    "        \n",
    "        #print(i, loss_hist[i])\n",
    "        \n",
    "        ##(4) Decay learning rate at the end of each epoch. \n",
    "        lr = lr * 0.9\n",
    "    #---------------------------------\n",
    "    \n",
    "    w_star = w\n",
    "    return w_star, w_hist, loss_hist  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate and print out the MSE and MAE values of SGD for the training and test sets (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6a3bc4f2c002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#train model using SGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mw_star_SGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hist_SGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_hist_SGD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m## add your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SGD' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 50\n",
    "\n",
    "#train model using SGD\n",
    "w_star_SGD, w_hist_SGD, loss_hist_SGD = SGD(X_train, y_train, lr = 0.0001, batch_size = batch_size, epoch = n_epochs)\n",
    "\n",
    "## add your code here\n",
    "#-----------------------\n",
    "#(1) print out the predicted wine quality values and the true quality \n",
    "# values of the first 10 data samples in the test dataset.  5 points\n",
    "\n",
    "\n",
    "\n",
    "#(2) model's MSE and MAE on the training set using w_star_SGD. 5 points\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#(3) model's MSE and MAE on the test set. 5 points\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot the loss curve of SGD returned from the training. (5 points)\n",
    "Plot the values in loss_hist_SGD. The horizontal axis is epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Plot the MSE curves on the training and test sets using different models (w_hist). (20 points)\n",
    "- each row in w_hist_SGD defines one linear model\n",
    "- apply each linear model to predict results for the training and test sets\n",
    "- calculate mse for both training and test sets.\n",
    "\n",
    "~~~python\n",
    "\n",
    "#sample code\n",
    "    for i in range(n_epochs):\n",
    "        #y_train_SGD_pred = # call predict using X_train and w_hist_SGD[i]\n",
    "        #y_test_SGD_pred = # call predict using X_test and w_hist_SGD[i]\n",
    "\n",
    "        #mse_SGD_train[i] = \n",
    "        #mse_SGD_test[i] = \n",
    "\n",
    "    #plot the mse curve on the training set\n",
    "    #plot the mse curve on the test set\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mse_SGD_train\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(n_epochs)\n\u001b[0;32m      2\u001b[0m mse_SGD_test\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(n_epochs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "mse_SGD_train=np.zeros(n_epochs)\n",
    "mse_SGD_test=np.zeros(n_epochs)\n",
    "\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

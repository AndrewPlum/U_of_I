{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c11fab3-8178-406e-92a3-5701a5d3dbc7",
   "metadata": {},
   "source": [
    "***Andrew Plum***<br/>\n",
    "***CS 474***<br/>\n",
    "***11/18/2024***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e4d2a-bc6e-4530-92cd-2c6d9db4425c",
   "metadata": {},
   "source": [
    "### HW 5: Build a many-to-one RNN for sentiment analysis\n",
    "\n",
    "In this homework, you will build a many-to-one RNN for sentiment analysis, i.e., classify reviews into two categories: positive (1) and negative(0).\n",
    "\n",
    "**Dataset**: \n",
    "\n",
    "    -IMDb Movie Reviews for binary sentiment classification\n",
    "    -A set of 50,000 reviews with labels. All reviews have been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1f1dad-2086-4144-a6b3-8d69791b5633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Feel free to import other necassary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tqdm import tqdm # you may need to install this package\n",
    "tqdm.pandas()\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b248e0-80bc-458e-944a-3ae010766d8d",
   "metadata": {},
   "source": [
    "#### 1. Load the dataset\n",
    "In the my_imdb.csv file, the 'review' column has preprocessed texts of user reviews, and the 'label' has binary catogories. 1 indicates positive comments, and 0 means negative comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c3d12a-d15d-402a-a7a6-7b48d1b5ad3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One reviewer mentioned watching Oz episode hoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production . The filming te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought wonderful way spend time hot summer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically family little boy Jake think zombie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei Love Time Money visually stunnin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  One reviewer mentioned watching Oz episode hoo...      1\n",
       "1  A wonderful little production . The filming te...      1\n",
       "2  I thought wonderful way spend time hot summer ...      1\n",
       "3  Basically family little boy Jake think zombie ...      0\n",
       "4  Petter Mattei Love Time Money visually stunnin...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read processed data\n",
    "df = pd.read_csv('my_imdb.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4e3f1-ac30-44da-b2eb-e0e6b37f6fcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Text vectorization\n",
    "Text vectorization converts texts into sequences of numeric values\n",
    "\n",
    "- The **vocabulary** is a set that contains all unique words in the dataset.\n",
    "- The **vectorizer** is dictionary that contains every word in the vocabulary set and its index\n",
    "- The **padding** creates vectors with fixex length, e.g., 256\n",
    "- Feel free to change the following code to use other text vectorization approaches, e.g., TF-IDF or BoW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2956ba-7e2a-4e46-8b9a-6822c1e0600f",
   "metadata": {},
   "source": [
    "2.1 Create the vocabulary and the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6451ccd-934f-404d-adda-7a2565464dfe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 1,\n",
       " ',': 2,\n",
       " 'I': 3,\n",
       " '-': 4,\n",
       " 'movie': 5,\n",
       " 'film': 6,\n",
       " 'The': 7,\n",
       " 'one': 8,\n",
       " '!': 9,\n",
       " 'like': 10,\n",
       " 'It': 11,\n",
       " '?': 12,\n",
       " 'time': 13,\n",
       " 'This': 14,\n",
       " 'good': 15,\n",
       " 'character': 16,\n",
       " 'story': 17,\n",
       " 'would': 18,\n",
       " 'get': 19,\n",
       " 'make': 20,\n",
       " 'see': 21,\n",
       " 'really': 22,\n",
       " 'even': 23,\n",
       " 'scene': 24,\n",
       " 'much': 25,\n",
       " 'well': 26,\n",
       " 'people': 27,\n",
       " 'bad': 28,\n",
       " 'great': 29,\n",
       " 'way': 30,\n",
       " 'show': 31,\n",
       " 'made': 32,\n",
       " 'thing': 33,\n",
       " 'first': 34,\n",
       " 'also': 35,\n",
       " 'could': 36,\n",
       " 'think': 37,\n",
       " 'life': 38,\n",
       " 'But': 39,\n",
       " 'know': 40,\n",
       " 'go': 41,\n",
       " 'And': 42,\n",
       " 'plot': 43,\n",
       " 'seen': 44,\n",
       " 'actor': 45,\n",
       " 'watch': 46,\n",
       " 'A': 47,\n",
       " 'say': 48,\n",
       " 'year': 49,\n",
       " 'love': 50,\n",
       " 'many': 51,\n",
       " 'end': 52,\n",
       " 'two': 53,\n",
       " 'acting': 54,\n",
       " 'look': 55,\n",
       " 'never': 56,\n",
       " 'There': 57,\n",
       " 'In': 58,\n",
       " 'little': 59,\n",
       " 'best': 60,\n",
       " 'ever': 61,\n",
       " 'better': 62,\n",
       " 'work': 63,\n",
       " 'If': 64,\n",
       " 'take': 65,\n",
       " 'come': 66,\n",
       " 'He': 67,\n",
       " 'find': 68,\n",
       " 'man': 69,\n",
       " 'part': 70,\n",
       " 'still': 71,\n",
       " 'something': 72,\n",
       " 'want': 73,\n",
       " 'give': 74,\n",
       " 'back': 75,\n",
       " 'lot': 76,\n",
       " 'real': 77,\n",
       " 'performance': 78,\n",
       " 'director': 79,\n",
       " 'play': 80,\n",
       " 'watching': 81,\n",
       " 'guy': 82,\n",
       " 'funny': 83,\n",
       " 'woman': 84,\n",
       " 'old': 85,\n",
       " 'role': 86,\n",
       " 'going': 87,\n",
       " 'actually': 88,\n",
       " 'though': 89,\n",
       " 'point': 90,\n",
       " 'cast': 91,\n",
       " 'nothing': 92,\n",
       " 'another': 93,\n",
       " 'minute': 94,\n",
       " 'thought': 95,\n",
       " 'fact': 96,\n",
       " 'feel': 97,\n",
       " 'girl': 98,\n",
       " 'comedy': 99,\n",
       " 'around': 100,\n",
       " 'quite': 101,\n",
       " 'got': 102,\n",
       " 'every': 103,\n",
       " 'action': 104,\n",
       " 'seems': 105,\n",
       " 'pretty': 106,\n",
       " 'horror': 107,\n",
       " 'day': 108,\n",
       " 'enough': 109,\n",
       " 'u': 110,\n",
       " 'You': 111,\n",
       " 'world': 112,\n",
       " 'bit': 113,\n",
       " 'right': 114,\n",
       " 'As': 115,\n",
       " 'line': 116,\n",
       " 'What': 117,\n",
       " 'They': 118,\n",
       " 'long': 119,\n",
       " 'fan': 120,\n",
       " 'series': 121,\n",
       " 'young': 122,\n",
       " 'friend': 123,\n",
       " 'must': 124,\n",
       " 'original': 125,\n",
       " 'set': 126,\n",
       " 'may': 127,\n",
       " 'always': 128,\n",
       " 'script': 129,\n",
       " 'saw': 130,\n",
       " 'music': 131,\n",
       " 'done': 132,\n",
       " 'least': 133,\n",
       " 'whole': 134,\n",
       " 'interesting': 135,\n",
       " 'family': 136,\n",
       " 'without': 137,\n",
       " 'almost': 138,\n",
       " 'big': 139,\n",
       " 'star': 140,\n",
       " 'try': 141,\n",
       " 'shot': 142,\n",
       " 'new': 143,\n",
       " 'far': 144,\n",
       " 'kind': 145,\n",
       " 'effect': 146,\n",
       " 'might': 147,\n",
       " 'making': 148,\n",
       " 'reason': 149,\n",
       " 'start': 150,\n",
       " 'anything': 151,\n",
       " 'kid': 152,\n",
       " 'TV': 153,\n",
       " 'book': 154,\n",
       " 'place': 155,\n",
       " 'put': 156,\n",
       " 'moment': 157,\n",
       " 'So': 158,\n",
       " 'She': 159,\n",
       " 'away': 160,\n",
       " 'probably': 161,\n",
       " 'last': 162,\n",
       " 'fun': 163,\n",
       " 'That': 164,\n",
       " 'idea': 165,\n",
       " 'audience': 166,\n",
       " 'found': 167,\n",
       " 'played': 168,\n",
       " 'screen': 169,\n",
       " 'child': 170,\n",
       " 'since': 171,\n",
       " 'need': 172,\n",
       " 'rather': 173,\n",
       " 'hard': 174,\n",
       " 'tell': 175,\n",
       " 'worst': 176,\n",
       " 'turn': 177,\n",
       " 'ending': 178,\n",
       " 'course': 179,\n",
       " 'looking': 180,\n",
       " 'DVD': 181,\n",
       " 'anyone': 182,\n",
       " 'When': 183,\n",
       " 'believe': 184,\n",
       " 'trying': 185,\n",
       " 'episode': 186,\n",
       " 'mean': 187,\n",
       " 'job': 188,\n",
       " 'yet': 189,\n",
       " 'different': 190,\n",
       " 'One': 191,\n",
       " 'especially': 192,\n",
       " 'main': 193,\n",
       " 'sense': 194,\n",
       " 'version': 195,\n",
       " 'American': 196,\n",
       " 'sure': 197,\n",
       " 'problem': 198,\n",
       " 'worth': 199,\n",
       " 'watched': 200,\n",
       " 'money': 201,\n",
       " 'help': 202,\n",
       " 'keep': 203,\n",
       " 'together': 204,\n",
       " 'someone': 205,\n",
       " 'said': 206,\n",
       " 'seem': 207,\n",
       " 'For': 208,\n",
       " 'laugh': 209,\n",
       " 'mind': 210,\n",
       " 'let': 211,\n",
       " 'three': 212,\n",
       " 'John': 213,\n",
       " 'wife': 214,\n",
       " 'hour': 215,\n",
       " 'true': 216,\n",
       " 'half': 217,\n",
       " 'All': 218,\n",
       " 'My': 219,\n",
       " 'lead': 220,\n",
       " 'left': 221,\n",
       " 'We': 222,\n",
       " 'short': 223,\n",
       " 'everything': 224,\n",
       " 'second': 225,\n",
       " 'special': 226,\n",
       " 'Not': 227,\n",
       " 'beautiful': 228,\n",
       " 'name': 229,\n",
       " 'else': 230,\n",
       " 'seeing': 231,\n",
       " 'budget': 232,\n",
       " 'viewer': 233,\n",
       " 'everyone': 234,\n",
       " 'later': 235,\n",
       " 'piece': 236,\n",
       " 'high': 237,\n",
       " 'used': 238,\n",
       " 'excellent': 239,\n",
       " 'However': 240,\n",
       " 'boy': 241,\n",
       " 'classic': 242,\n",
       " 'face': 243,\n",
       " 'production': 244,\n",
       " 'read': 245,\n",
       " 'le': 246,\n",
       " 'sound': 247,\n",
       " 'completely': 248,\n",
       " 'father': 249,\n",
       " 'camera': 250,\n",
       " 'death': 251,\n",
       " 'simply': 252,\n",
       " 'nice': 253,\n",
       " 'eye': 254,\n",
       " 'Hollywood': 255,\n",
       " 'top': 256,\n",
       " 'couple': 257,\n",
       " 'human': 258,\n",
       " 'word': 259,\n",
       " 'poor': 260,\n",
       " 'home': 261,\n",
       " 'use': 262,\n",
       " 'song': 263,\n",
       " 'hand': 264,\n",
       " 'low': 265,\n",
       " 'video': 266,\n",
       " 'head': 267,\n",
       " 'either': 268,\n",
       " 'rest': 269,\n",
       " 'boring': 270,\n",
       " 'wrong': 271,\n",
       " 'along': 272,\n",
       " 'night': 273,\n",
       " 'enjoy': 274,\n",
       " 'person': 275,\n",
       " 'recommend': 276,\n",
       " 'school': 277,\n",
       " 'style': 278,\n",
       " 'given': 279,\n",
       " 'stupid': 280,\n",
       " 'picture': 281,\n",
       " 'men': 282,\n",
       " 'writer': 283,\n",
       " 'understand': 284,\n",
       " 'truly': 285,\n",
       " 'war': 286,\n",
       " 'came': 287,\n",
       " 'sort': 288,\n",
       " 'sex': 289,\n",
       " 'dialogue': 290,\n",
       " 'No': 291,\n",
       " 'flick': 292,\n",
       " 'getting': 293,\n",
       " 'awful': 294,\n",
       " 'run': 295,\n",
       " 'house': 296,\n",
       " 'instead': 297,\n",
       " 'full': 298,\n",
       " 'joke': 299,\n",
       " 'case': 300,\n",
       " 'attempt': 301,\n",
       " 'hope': 302,\n",
       " 'black': 303,\n",
       " 'game': 304,\n",
       " 'kill': 305,\n",
       " 'playing': 306,\n",
       " 'care': 307,\n",
       " 'sequence': 308,\n",
       " 'Well': 309,\n",
       " 'next': 310,\n",
       " 'After': 311,\n",
       " 'title': 312,\n",
       " 'act': 313,\n",
       " 'At': 314,\n",
       " 'To': 315,\n",
       " 'terrible': 316,\n",
       " 'however': 317,\n",
       " 'small': 318,\n",
       " 'remember': 319,\n",
       " 'although': 320,\n",
       " 'maybe': 321,\n",
       " 'review': 322,\n",
       " 'Even': 323,\n",
       " 'fall': 324,\n",
       " 'mother': 325,\n",
       " 'often': 326,\n",
       " 'drama': 327,\n",
       " 'wonderful': 328,\n",
       " 'others': 329,\n",
       " 'quality': 330,\n",
       " 'perfect': 331,\n",
       " 'early': 332,\n",
       " 'written': 333,\n",
       " 'went': 334,\n",
       " 'THE': 335,\n",
       " 'example': 336,\n",
       " 'feeling': 337,\n",
       " 'become': 338,\n",
       " 'liked': 339,\n",
       " 'actress': 340,\n",
       " 'entertaining': 341,\n",
       " 'called': 342,\n",
       " 'felt': 343,\n",
       " 'car': 344,\n",
       " 'supposed': 345,\n",
       " 'matter': 346,\n",
       " 'entire': 347,\n",
       " 'waste': 348,\n",
       " 'side': 349,\n",
       " 'art': 350,\n",
       " 'cinema': 351,\n",
       " 'Mr': 352,\n",
       " 'absolutely': 353,\n",
       " 'worse': 354,\n",
       " 'lack': 355,\n",
       " 'feature': 356,\n",
       " 'beginning': 357,\n",
       " 'live': 358,\n",
       " 'comment': 359,\n",
       " 'Don': 360,\n",
       " 'His': 361,\n",
       " 'certainly': 362,\n",
       " 'definitely': 363,\n",
       " 'begin': 364,\n",
       " 'type': 365,\n",
       " 'direction': 366,\n",
       " 'loved': 367,\n",
       " 'favorite': 368,\n",
       " 'Then': 369,\n",
       " 'humor': 370,\n",
       " 'seemed': 371,\n",
       " 'While': 372,\n",
       " 'wanted': 373,\n",
       " 'several': 374,\n",
       " 'killer': 375,\n",
       " 'son': 376,\n",
       " 'Also': 377,\n",
       " 'fight': 378,\n",
       " 'change': 379,\n",
       " 'relationship': 380,\n",
       " 'already': 381,\n",
       " 'becomes': 382,\n",
       " 'voice': 383,\n",
       " 'totally': 384,\n",
       " 'based': 385,\n",
       " 'able': 386,\n",
       " 'dead': 387,\n",
       " 'genre': 388,\n",
       " 'hit': 389,\n",
       " 'Of': 390,\n",
       " 'Why': 391,\n",
       " 'experience': 392,\n",
       " 'guess': 393,\n",
       " 'heart': 394,\n",
       " 'Now': 395,\n",
       " 'hero': 396,\n",
       " 'Some': 397,\n",
       " 'brother': 398,\n",
       " 'Michael': 399,\n",
       " 'number': 400,\n",
       " 'writing': 401,\n",
       " 'lost': 402,\n",
       " 'meet': 403,\n",
       " 'daughter': 404,\n",
       " 'murder': 405,\n",
       " 'group': 406,\n",
       " 'fine': 407,\n",
       " 'throughout': 408,\n",
       " 'town': 409,\n",
       " 'history': 410,\n",
       " 'stop': 411,\n",
       " 'talent': 412,\n",
       " 'cut': 413,\n",
       " 'move': 414,\n",
       " 'etc': 415,\n",
       " 'today': 416,\n",
       " 'past': 417,\n",
       " 'How': 418,\n",
       " 'enjoyed': 419,\n",
       " 'gave': 420,\n",
       " 'close': 421,\n",
       " 'final': 422,\n",
       " 'stuff': 423,\n",
       " 'event': 424,\n",
       " 'situation': 425,\n",
       " 'level': 426,\n",
       " 'amazing': 427,\n",
       " 'finally': 428,\n",
       " 'credit': 429,\n",
       " 'theme': 430,\n",
       " 'expect': 431,\n",
       " 'horrible': 432,\n",
       " 'call': 433,\n",
       " 'behind': 434,\n",
       " 'stand': 435,\n",
       " 'age': 436,\n",
       " 'self': 437,\n",
       " 'late': 438,\n",
       " 'With': 439,\n",
       " 'killed': 440,\n",
       " 'white': 441,\n",
       " 'chance': 442,\n",
       " 'thinking': 443,\n",
       " 'perhaps': 444,\n",
       " 'evil': 445,\n",
       " 'wonder': 446,\n",
       " 'question': 447,\n",
       " 'New': 448,\n",
       " 'brilliant': 449,\n",
       " 'score': 450,\n",
       " 'decent': 451,\n",
       " 'body': 452,\n",
       " 'known': 453,\n",
       " 'took': 454,\n",
       " 'view': 455,\n",
       " 'directed': 456,\n",
       " 'Just': 457,\n",
       " 'happens': 458,\n",
       " 'heard': 459,\n",
       " 'dark': 460,\n",
       " 'career': 461,\n",
       " 'element': 462,\n",
       " 'novel': 463,\n",
       " 'documentary': 464,\n",
       " 'save': 465,\n",
       " 'light': 466,\n",
       " 'slow': 467,\n",
       " 'interest': 468,\n",
       " 'add': 469,\n",
       " 'involved': 470,\n",
       " 'On': 471,\n",
       " 'told': 472,\n",
       " 'husband': 473,\n",
       " 'effort': 474,\n",
       " 'coming': 475,\n",
       " 'power': 476,\n",
       " 'happen': 477,\n",
       " 'country': 478,\n",
       " 'David': 479,\n",
       " 'looked': 480,\n",
       " 'extremely': 481,\n",
       " 'opinion': 482,\n",
       " 'wish': 483,\n",
       " 'including': 484,\n",
       " 'B': 485,\n",
       " 'reality': 486,\n",
       " 'order': 487,\n",
       " 'leave': 488,\n",
       " 'obvious': 489,\n",
       " 'violence': 490,\n",
       " 'soon': 491,\n",
       " 'James': 492,\n",
       " 'sequel': 493,\n",
       " 'ago': 494,\n",
       " 'shown': 495,\n",
       " 'serious': 496,\n",
       " 'twist': 497,\n",
       " 'Robert': 498,\n",
       " 'particularly': 499,\n",
       " 'happened': 500,\n",
       " 'hilarious': 501,\n",
       " 'strong': 502,\n",
       " 'crap': 503,\n",
       " 'deal': 504,\n",
       " 'musical': 505,\n",
       " 'complete': 506,\n",
       " 'obviously': 507,\n",
       " 'talk': 508,\n",
       " 'Oh': 509,\n",
       " 'taken': 510,\n",
       " 'except': 511,\n",
       " 'value': 512,\n",
       " 'English': 513,\n",
       " 'female': 514,\n",
       " 'simple': 515,\n",
       " 'released': 516,\n",
       " 'opening': 517,\n",
       " 'across': 518,\n",
       " 'room': 519,\n",
       " 'Although': 520,\n",
       " 'thriller': 521,\n",
       " 'highly': 522,\n",
       " 'cool': 523,\n",
       " 'sometimes': 524,\n",
       " 'saying': 525,\n",
       " 'dialog': 526,\n",
       " 'exactly': 527,\n",
       " 'annoying': 528,\n",
       " 'whose': 529,\n",
       " 'monster': 530,\n",
       " 'theater': 531,\n",
       " 'sad': 532,\n",
       " 'started': 533,\n",
       " 'local': 534,\n",
       " 'turned': 535,\n",
       " 'possible': 536,\n",
       " 'cinematography': 537,\n",
       " 'important': 538,\n",
       " 'cop': 539,\n",
       " 'gore': 540,\n",
       " 'OK': 541,\n",
       " 'comic': 542,\n",
       " 'ridiculous': 543,\n",
       " 'message': 544,\n",
       " 'class': 545,\n",
       " 'Man': 546,\n",
       " 'sister': 547,\n",
       " 'living': 548,\n",
       " 'First': 549,\n",
       " 'open': 550,\n",
       " 'alone': 551,\n",
       " 'running': 552,\n",
       " 'despite': 553,\n",
       " 'police': 554,\n",
       " 'attention': 555,\n",
       " 'taking': 556,\n",
       " 'usual': 557,\n",
       " 'somewhat': 558,\n",
       " 'disappointed': 559,\n",
       " 'rating': 560,\n",
       " 'huge': 561,\n",
       " 'knew': 562,\n",
       " 'figure': 563,\n",
       " 'silly': 564,\n",
       " 'single': 565,\n",
       " 'talking': 566,\n",
       " 'blood': 567,\n",
       " 'mention': 568,\n",
       " 'surprise': 569,\n",
       " 'result': 570,\n",
       " 'member': 571,\n",
       " 'release': 572,\n",
       " 'producer': 573,\n",
       " 'non': 574,\n",
       " 'usually': 575,\n",
       " 'Jack': 576,\n",
       " 'hate': 577,\n",
       " 'dream': 578,\n",
       " 'mostly': 579,\n",
       " 'subject': 580,\n",
       " 'cheap': 581,\n",
       " 'middle': 582,\n",
       " 'parent': 583,\n",
       " 'team': 584,\n",
       " 'image': 585,\n",
       " 'British': 586,\n",
       " 'due': 587,\n",
       " 'television': 588,\n",
       " 'stay': 589,\n",
       " 'modern': 590,\n",
       " 'drug': 591,\n",
       " 'filmmaker': 592,\n",
       " 'happy': 593,\n",
       " 'form': 594,\n",
       " 'scary': 595,\n",
       " 'major': 596,\n",
       " 'From': 597,\n",
       " 'soundtrack': 598,\n",
       " 'viewing': 599,\n",
       " 'crime': 600,\n",
       " 'villain': 601,\n",
       " 'hold': 602,\n",
       " 'Maybe': 603,\n",
       " 'George': 604,\n",
       " 'appears': 605,\n",
       " 'doubt': 606,\n",
       " 'tale': 607,\n",
       " 'S': 608,\n",
       " 'upon': 609,\n",
       " 'seriously': 610,\n",
       " 'predictable': 611,\n",
       " 'dog': 612,\n",
       " 'moving': 613,\n",
       " 'enjoyable': 614,\n",
       " 'Yes': 615,\n",
       " 'strange': 616,\n",
       " 'aspect': 617,\n",
       " 'killing': 618,\n",
       " 'four': 619,\n",
       " 'Unfortunately': 620,\n",
       " 'romantic': 621,\n",
       " 'beyond': 622,\n",
       " 'similar': 623,\n",
       " 'hell': 624,\n",
       " 'giving': 625,\n",
       " 'break': 626,\n",
       " 'God': 627,\n",
       " 'clear': 628,\n",
       " 'future': 629,\n",
       " 'near': 630,\n",
       " 'clearly': 631,\n",
       " 'straight': 632,\n",
       " 'emotion': 633,\n",
       " 'entertainment': 634,\n",
       " 'easily': 635,\n",
       " 'surprised': 636,\n",
       " 'bring': 637,\n",
       " 'tried': 638,\n",
       " 'working': 639,\n",
       " 'showing': 640,\n",
       " 'city': 641,\n",
       " 'storyline': 642,\n",
       " 'adult': 643,\n",
       " 'setting': 644,\n",
       " 'ten': 645,\n",
       " 'none': 646,\n",
       " 'certain': 647,\n",
       " 'bunch': 648,\n",
       " 'suspense': 649,\n",
       " 'zombie': 650,\n",
       " 'dull': 651,\n",
       " 'named': 652,\n",
       " 'standard': 653,\n",
       " 'present': 654,\n",
       " 'buy': 655,\n",
       " 'victim': 656,\n",
       " 'fast': 657,\n",
       " 'season': 658,\n",
       " 'supporting': 659,\n",
       " 'filmed': 660,\n",
       " 'material': 661,\n",
       " 'Richard': 662,\n",
       " 'detail': 663,\n",
       " 'kept': 664,\n",
       " 'within': 665,\n",
       " 'student': 666,\n",
       " 'French': 667,\n",
       " 'Peter': 668,\n",
       " 'period': 669,\n",
       " 'easy': 670,\n",
       " 'realistic': 671,\n",
       " 'Dr': 672,\n",
       " 'th': 673,\n",
       " 'typical': 674,\n",
       " 'overall': 675,\n",
       " 'An': 676,\n",
       " 'Its': 677,\n",
       " 'Paul': 678,\n",
       " 'lady': 679,\n",
       " 'Oscar': 680,\n",
       " 'five': 681,\n",
       " 'gun': 682,\n",
       " 'issue': 683,\n",
       " 'premise': 684,\n",
       " 'editing': 685,\n",
       " 'return': 686,\n",
       " 'actual': 687,\n",
       " 'believable': 688,\n",
       " 'Like': 689,\n",
       " 'nearly': 690,\n",
       " 'using': 691,\n",
       " 'brought': 692,\n",
       " 'famous': 693,\n",
       " 'cartoon': 694,\n",
       " 'mystery': 695,\n",
       " 'among': 696,\n",
       " 'whether': 697,\n",
       " 'animation': 698,\n",
       " 'follow': 699,\n",
       " 'offer': 700,\n",
       " 'soldier': 701,\n",
       " 'Who': 702,\n",
       " 'fit': 703,\n",
       " 'Is': 704,\n",
       " 'America': 705,\n",
       " 'copy': 706,\n",
       " 'O': 707,\n",
       " 'Tom': 708,\n",
       " 'hear': 709,\n",
       " 'background': 710,\n",
       " 'greatest': 711,\n",
       " 'basically': 712,\n",
       " 'die': 713,\n",
       " 'stage': 714,\n",
       " 'atmosphere': 715,\n",
       " 'male': 716,\n",
       " 'Most': 717,\n",
       " 'weak': 718,\n",
       " 'masterpiece': 719,\n",
       " 'expected': 720,\n",
       " 'dance': 721,\n",
       " 'apparently': 722,\n",
       " 'average': 723,\n",
       " 'particular': 724,\n",
       " 'York': 725,\n",
       " 'fantastic': 726,\n",
       " 'learn': 727,\n",
       " 'sit': 728,\n",
       " 'Overall': 729,\n",
       " 'These': 730,\n",
       " 'pay': 731,\n",
       " 'battle': 732,\n",
       " 'rate': 733,\n",
       " 'decided': 734,\n",
       " 'Her': 735,\n",
       " 'note': 736,\n",
       " 'truth': 737,\n",
       " 'leaf': 738,\n",
       " 'choice': 739,\n",
       " 'romance': 740,\n",
       " 'miss': 741,\n",
       " 'rent': 742,\n",
       " 'escape': 743,\n",
       " 'difficult': 744,\n",
       " 'cover': 745,\n",
       " 'cause': 746,\n",
       " 'needed': 747,\n",
       " 'society': 748,\n",
       " 'box': 749,\n",
       " 'poorly': 750,\n",
       " 'Lee': 751,\n",
       " 'emotional': 752,\n",
       " 'acted': 753,\n",
       " 'footage': 754,\n",
       " 'gone': 755,\n",
       " 'crew': 756,\n",
       " 'accent': 757,\n",
       " 'focus': 758,\n",
       " 'D': 759,\n",
       " 'became': 760,\n",
       " 'week': 761,\n",
       " 'girlfriend': 762,\n",
       " 'street': 763,\n",
       " 'By': 764,\n",
       " 'forced': 765,\n",
       " 'King': 766,\n",
       " 'force': 767,\n",
       " 'yes': 768,\n",
       " 'write': 769,\n",
       " 'sexual': 770,\n",
       " 'War': 771,\n",
       " 'NOT': 772,\n",
       " 'free': 773,\n",
       " 'memorable': 774,\n",
       " 'space': 775,\n",
       " 'forward': 776,\n",
       " 'lame': 777,\n",
       " 'walk': 778,\n",
       " 'wait': 779,\n",
       " 'gay': 780,\n",
       " 'anyway': 781,\n",
       " 'Here': 782,\n",
       " 'location': 783,\n",
       " 'reading': 784,\n",
       " 'screenplay': 785,\n",
       " 'interested': 786,\n",
       " 'somehow': 787,\n",
       " 'lover': 788,\n",
       " 'T': 789,\n",
       " 'nature': 790,\n",
       " 'perfectly': 791,\n",
       " 'cheesy': 792,\n",
       " 'mess': 793,\n",
       " 'previous': 794,\n",
       " 'check': 795,\n",
       " 'Japanese': 796,\n",
       " 'touch': 797,\n",
       " 'forget': 798,\n",
       " 'personal': 799,\n",
       " 'maker': 800,\n",
       " 'quickly': 801,\n",
       " 'development': 802,\n",
       " 'third': 803,\n",
       " 'possibly': 804,\n",
       " 'imagine': 805,\n",
       " 'win': 806,\n",
       " 'worked': 807,\n",
       " 'superb': 808,\n",
       " 'please': 809,\n",
       " 'Disney': 810,\n",
       " 'realize': 811,\n",
       " 'badly': 812,\n",
       " 'remake': 813,\n",
       " 'general': 814,\n",
       " 'business': 815,\n",
       " 'unique': 816,\n",
       " 'deep': 817,\n",
       " 'earlier': 818,\n",
       " 'older': 819,\n",
       " 'Joe': 820,\n",
       " 'pick': 821,\n",
       " 'front': 822,\n",
       " 'sorry': 823,\n",
       " 'powerful': 824,\n",
       " 'IMDb': 825,\n",
       " 'amount': 826,\n",
       " 'appear': 827,\n",
       " 'dramatic': 828,\n",
       " 'weird': 829,\n",
       " 'success': 830,\n",
       " 'term': 831,\n",
       " 'match': 832,\n",
       " 'brings': 833,\n",
       " 'inside': 834,\n",
       " 'shame': 835,\n",
       " 'beauty': 836,\n",
       " 'towards': 837,\n",
       " 'teen': 838,\n",
       " 'shoot': 839,\n",
       " 'various': 840,\n",
       " 'admit': 841,\n",
       " 'incredibly': 842,\n",
       " 'fantasy': 843,\n",
       " 'Or': 844,\n",
       " 'hot': 845,\n",
       " 'directing': 846,\n",
       " 'portrayed': 847,\n",
       " 'doctor': 848,\n",
       " 'adventure': 849,\n",
       " 'creepy': 850,\n",
       " 'spent': 851,\n",
       " 'era': 852,\n",
       " 'party': 853,\n",
       " 'eventually': 854,\n",
       " 'political': 855,\n",
       " 'state': 856,\n",
       " 'memory': 857,\n",
       " 'Another': 858,\n",
       " 'fails': 859,\n",
       " 'total': 860,\n",
       " 'ask': 861,\n",
       " 'store': 862,\n",
       " 'portrayal': 863,\n",
       " 'costume': 864,\n",
       " 'fairly': 865,\n",
       " 'animal': 866,\n",
       " 'William': 867,\n",
       " 'deserves': 868,\n",
       " 'air': 869,\n",
       " 'wasted': 870,\n",
       " 'dumb': 871,\n",
       " 'leading': 872,\n",
       " 'telling': 873,\n",
       " 'concept': 874,\n",
       " 'studio': 875,\n",
       " 'Great': 876,\n",
       " 'rock': 877,\n",
       " 'list': 878,\n",
       " 'ability': 879,\n",
       " 'create': 880,\n",
       " 'meant': 881,\n",
       " 'fighting': 882,\n",
       " 'manages': 883,\n",
       " 'agree': 884,\n",
       " 'rich': 885,\n",
       " 'appearance': 886,\n",
       " 'ended': 887,\n",
       " 'expecting': 888,\n",
       " 'cry': 889,\n",
       " 'plenty': 890,\n",
       " 'depth': 891,\n",
       " 'company': 892,\n",
       " 'mistake': 893,\n",
       " 'secret': 894,\n",
       " 'indeed': 895,\n",
       " 'player': 896,\n",
       " 'missing': 897,\n",
       " 'water': 898,\n",
       " 'outside': 899,\n",
       " 'caught': 900,\n",
       " 'whatever': 901,\n",
       " 'talented': 902,\n",
       " 'Let': 903,\n",
       " 'pace': 904,\n",
       " 'German': 905,\n",
       " 'trouble': 906,\n",
       " 'Instead': 907,\n",
       " 'fire': 908,\n",
       " 'laughing': 909,\n",
       " 'crazy': 910,\n",
       " 'large': 911,\n",
       " 'language': 912,\n",
       " 'cute': 913,\n",
       " 'clever': 914,\n",
       " 'Italian': 915,\n",
       " 'plain': 916,\n",
       " 'created': 917,\n",
       " 'potential': 918,\n",
       " 'Black': 919,\n",
       " 'produced': 920,\n",
       " 'flat': 921,\n",
       " 'brain': 922,\n",
       " 'pull': 923,\n",
       " 'tension': 924,\n",
       " 'culture': 925,\n",
       " 'recently': 926,\n",
       " 'plan': 927,\n",
       " 'creature': 928,\n",
       " 'project': 929,\n",
       " 'mentioned': 930,\n",
       " 'wrote': 931,\n",
       " 'vampire': 932,\n",
       " 'Scott': 933,\n",
       " 'following': 934,\n",
       " 'control': 935,\n",
       " 'unless': 936,\n",
       " 'western': 937,\n",
       " 'missed': 938,\n",
       " 'fear': 939,\n",
       " 'stick': 940,\n",
       " 'nudity': 941,\n",
       " 'odd': 942,\n",
       " 'respect': 943,\n",
       " 'familiar': 944,\n",
       " 'hardly': 945,\n",
       " 'Good': 946,\n",
       " 'waiting': 947,\n",
       " 'slightly': 948,\n",
       " 'hole': 949,\n",
       " 'co': 950,\n",
       " 'Bill': 951,\n",
       " 'band': 952,\n",
       " 'drive': 953,\n",
       " 'Director': 954,\n",
       " 'World': 955,\n",
       " 'speak': 956,\n",
       " 'purpose': 957,\n",
       " 'public': 958,\n",
       " 'visual': 959,\n",
       " 'casting': 960,\n",
       " 'Movie': 961,\n",
       " 'filled': 962,\n",
       " 'decides': 963,\n",
       " 'us': 964,\n",
       " 'convincing': 965,\n",
       " 'popular': 966,\n",
       " 'throw': 967,\n",
       " 'Perhaps': 968,\n",
       " 'married': 969,\n",
       " 'Jane': 970,\n",
       " 'entirely': 971,\n",
       " 'Do': 972,\n",
       " 'fi': 973,\n",
       " 'door': 974,\n",
       " 'attack': 975,\n",
       " 'track': 976,\n",
       " 'color': 977,\n",
       " 'extra': 978,\n",
       " 'follows': 979,\n",
       " 'positive': 980,\n",
       " 'bored': 981,\n",
       " 'meaning': 982,\n",
       " 'appreciate': 983,\n",
       " 'died': 984,\n",
       " 'Star': 985,\n",
       " 'answer': 986,\n",
       " 'pure': 987,\n",
       " 'social': 988,\n",
       " 'belief': 989,\n",
       " 'unfortunately': 990,\n",
       " 'taste': 991,\n",
       " 'common': 992,\n",
       " 'intelligent': 993,\n",
       " 'younger': 994,\n",
       " 'former': 995,\n",
       " 'catch': 996,\n",
       " 'hair': 997,\n",
       " 'spirit': 998,\n",
       " 'building': 999,\n",
       " 'office': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all words\n",
    "text = df.review.values\n",
    "words = ' '.join(text)\n",
    "words = words.split() \n",
    "\n",
    "# build vocabulary\n",
    "vocab = sorted(Counter(words), key=Counter(words).get, reverse=True)\n",
    "ID2W = dict(enumerate(vocab, 1))\n",
    "ID2W[0] = '<PAD>' # special word for paddding purpose, and the index is 0\n",
    "vectorizer = {word: ID for ID, word in ID2W.items()}\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b342aa-3a75-4497-92b2-ebbe5696ee89",
   "metadata": {
    "tags": []
   },
   "source": [
    "2.2 Sample results of vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e079a8-0dfb-4267-b452-a7fe5c076955",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample: One reviewer mentioned watching Oz episode hooked . They right , exactly happened . The first thing struck Oz brutality unflinching scene violence , set right word GO . Trust , show faint hearted timid . This show pull punch regard drug , sex violence . Its hardcore , classic use word . It called OZ nickname given Oswald Maximum Security State Penitentary . It focus mainly Emerald City , experimental section prison cell glass front face inwards , privacy high agenda . Em City home many . . Aryans , Muslims , gangsta , Latinos , Christians , Italians , Irish . . . . scuffle , death stare , dodgy dealing shady agreement never far away . I would say main appeal show due fact go show dare . Forget pretty picture painted mainstream audience , forget charm , forget romance . . . OZ mess around . The first episode I ever saw struck nasty surreal , I say I ready , I watched , I developed taste Oz , got accustomed high level graphic violence . Not violence , injustice crooked guard sold nickel , inmate kill order get away , well mannered , middle class inmate turned prison bitch due lack street skill prison experience Watching Oz , may become comfortable uncomfortable viewing . . . . thats get touch darker side .\n",
      "Coverted vector: 228 [191, 1083, 930, 81, 3724, 186, 3030, 1, 118, 114, 2, 527, 500, 1, 7, 34, 33, 3086, 3724, 5192, 15234, 24, 490, 2, 126, 114, 259, 7473, 1, 5550, 2, 31, 6620, 2169, 11285, 1, 14, 31, 923, 2106, 2177, 591, 2, 289, 490, 1, 677, 3805, 2, 242, 262, 259, 1, 11, 342, 9227, 10121, 279, 14927, 22637, 12715, 4100, 55917, 1, 11, 758, 1332, 29362, 1094, 2, 4665, 1963, 1105, 2317, 2079, 822, 243, 55918, 2, 16586, 237, 4338, 1, 18349, 1094, 261, 51, 1, 1, 25451, 2, 8327, 2, 12516, 2, 15235, 2, 5060, 2, 7897, 2, 2344, 1, 1, 1, 1, 22638, 2, 251, 4076, 2, 7347, 1674, 8574, 9867, 56, 144, 160, 1, 3, 18, 48, 193, 1066, 31, 587, 96, 41, 31, 2759, 1, 4156, 106, 281, 4057, 2373, 166, 2, 798, 1179, 2, 798, 740, 1, 1, 1, 9227, 793, 100, 1, 7, 34, 186, 3, 61, 130, 3086, 1520, 2117, 2, 3, 48, 3, 1457, 2, 3, 200, 2, 3, 1283, 991, 3724, 2, 102, 10122, 237, 426, 1295, 490, 1, 227, 490, 2, 6231, 7537, 2219, 2851, 21863, 2, 5136, 305, 487, 19, 160, 2, 26, 7405, 2, 582, 545, 5136, 535, 1105, 5438, 587, 355, 763, 1175, 1105, 392, 1947, 3724, 2, 127, 338, 3639, 3156, 599, 1, 1, 1, 1, 1645, 19, 797, 3941, 349, 1]\n"
     ]
    }
   ],
   "source": [
    "print('Text sample:', text[0])\n",
    "num_vec = [vectorizer[w] for w in text[0].split()]\n",
    "print('Coverted vector:',  len(num_vec), num_vec,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910d1dc-9ed3-45ee-bbd5-4a1eb4b9475c",
   "metadata": {
    "tags": []
   },
   "source": [
    "2.3 Apply vectorization to the whole dataset\n",
    "- Apply padding to create sequences with fixed length\n",
    "- The final dataset with with numeric features is (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b1774a-2f85-4968-9be8-c585b86cde90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 50000/50000 [00:02<00:00, 18362.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vectors: (50000, 256)\n",
      "Labels: (50000,)\n"
     ]
    }
   ],
   "source": [
    "# text vectorization and padding\n",
    "vecs = [[vectorizer[w] for w in r.split()] for r in tqdm(text)] # vecs contains the vectorized reviews of varying length\n",
    "seq_length = 256\n",
    "    \n",
    "X = np.full((len(vecs), seq_length), 0, dtype=int) # create matrix whose shape is (50000, 256) initialized with 0\n",
    "for i, vec in enumerate(vecs): \n",
    "    X[i, :len(vec)] = np.array(vec)[:seq_length] # of each vectorized review, grab only up to the first 256 values and store in the row of X\n",
    "\n",
    "print('Text vectors:', X.shape)\n",
    "Y = df.label.to_numpy()\n",
    "print('Labels:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59002fa-fbe6-4100-8df0-35a0262234ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vector:\n",
      " 291\n",
      "Results with padding:\n",
      " [  191  1083   930    81  3724   186  3030     1   118   114     2   527\n",
      "   500     1     7    34    33  3086  3724  5192 15234    24   490     2\n",
      "   126   114   259  7473     1  5550     2    31  6620  2169 11285     1\n",
      "    14    31   923  2106  2177   591     2   289   490     1   677  3805\n",
      "     2   242   262   259     1    11   342  9227 10121   279 14927 22637\n",
      " 12715  4100 55917     1    11   758  1332 29362  1094     2  4665  1963\n",
      "  1105  2317  2079   822   243 55918     2 16586   237  4338     1 18349\n",
      "  1094   261    51     1     1 25451     2  8327     2 12516     2 15235\n",
      "     2  5060     2  7897     2  2344     1     1     1     1 22638     2\n",
      "   251  4076     2  7347  1674  8574  9867    56   144   160     1     3\n",
      "    18    48   193  1066    31   587    96    41    31  2759     1  4156\n",
      "   106   281  4057  2373   166     2   798  1179     2   798   740     1\n",
      "     1     1  9227   793   100     1     7    34   186     3    61   130\n",
      "  3086  1520  2117     2     3    48     3  1457     2     3   200     2\n",
      "     3  1283   991  3724     2   102 10122   237   426  1295   490     1\n",
      "   227   490     2  6231  7537  2219  2851 21863     2  5136   305   487\n",
      "    19   160     2    26  7405     2   582   545  5136   535  1105  5438\n",
      "   587   355   763  1175  1105   392  1947  3724     2   127   338  3639\n",
      "  3156   599     1     1     1     1  1645    19   797  3941   349     1\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print('Original vector:\\n', vec[0])\n",
    "print('Results with padding:\\n', X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb29b4a-eea4-42e1-89c5-5e860bcf31f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. Prepare the training, validation and test sets. 15 points\n",
    "- Use 50% data for training, 20% for validation, and 30% for testing\n",
    "- Set the batch size to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891f69a8-443b-487d-b466-8b9c56ce8cab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "(25000, 256)\n",
      "(25000,)\n",
      "\n",
      "Validation set:\n",
      "(10000, 256)\n",
      "(10000,)\n",
      "\n",
      "Test set:\n",
      "(15000, 256)\n",
      "(15000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.5, random_state = 0)\n",
    "X_validation, X_test, Y_validation, Y_test = train_test_split(X_test, Y_test, test_size = 0.6, random_state = 0)\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print()\n",
    "print(\"Validation set:\")\n",
    "print(X_validation.shape)\n",
    "print(Y_validation.shape)\n",
    "print()\n",
    "print(\"Test set:\")\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print()\n",
    "\n",
    "# Use TensorDataset and DataLoader\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "Y_train_tensor = torch.tensor(Y_train)\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "\n",
    "X_validation_tensor = torch.tensor(X_validation)\n",
    "Y_validation_tensor = torch.tensor(Y_validation)\n",
    "validation_dataset = TensorDataset(X_validation_tensor, Y_validation_tensor)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test)\n",
    "Y_test_tensor = torch.tensor(Y_test)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "\"\"\"\n",
    "train_dataloader = DataLoader(train_dataset, shuffle = True)\n",
    "validation_dataloader = DataLoader(validation_dataset, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle = True)\n",
    "\"\"\"\n",
    "\n",
    "#\"\"\"\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle = True)\n",
    "#\"\"\"\n",
    "\n",
    "# batch_size set down below with other hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6262dd-14ff-484f-bec8-6f31df046286",
   "metadata": {},
   "source": [
    "#### 4. Build a many-to-one RNN. 40 points\n",
    "- GRU or LSTM is preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9a5770-54b0-4175-a17e-97a03f499608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #h0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size) #  X.shape[1] or 0 or batch_size) !!!!!!!!\n",
    "        #c0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size) #  X.shape[1] or ) !!!!!!!!\n",
    "        \n",
    "        #out, _ = self.lstm(X, (h0, c0))\n",
    "        #X = self.embedding = torch.nn.Embedding(input_size, 256)\n",
    "        \n",
    "        out, (hidden, cell) = self.lstm(X)\n",
    "        \n",
    "        #out = out[:, -1, :]\n",
    "        #out = out[:, :]\n",
    "        out = self.linear(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    #CHECK!!!\n",
    "    def train_model(self, train_dataloader, validation_dataloader, num_epochs, loss_kind, optimizer):\n",
    "        train_loss = []\n",
    "        validation_loss = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            epoch_train_losses = []\n",
    "            for inputs, labels in train_dataloader:\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs).squeeze()\n",
    "                tr_loss = loss_kind(outputs, labels)\n",
    "                tr_loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_train_losses.append(tr_loss.item())\n",
    "            \n",
    "            self.eval()\n",
    "            epoch_validation_losses = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in validation_dataloader:\n",
    "                    inputs = inputs.float()\n",
    "                    labels = labels.float()\n",
    "                    outputs = self(inputs).squeeze()\n",
    "                    val_loss = loss_kind(outputs, labels)\n",
    "                    epoch_validation_losses.append(val_loss.item())\n",
    "               \n",
    "            avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "            avg_validation_loss = sum(epoch_validation_losses) / len(epoch_validation_losses)\n",
    "            train_loss.append(avg_train_loss)\n",
    "            validation_loss.append(avg_validation_loss)\n",
    "            \n",
    "            print(\"Epoch: \", epoch + 1)\n",
    "            print(\"Training loss: \", tr_loss.item())\n",
    "            print(\"Validation loss: \", val_loss.item())\n",
    "            \n",
    "        return train_loss, validation_loss\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "        y_hat = []\n",
    "        y = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                outputs = self(inputs).squeeze()\n",
    "                predictions = (outputs > 0.5).float()\n",
    "                y_hat.extend(predictions.numpy())\n",
    "                y.extend(labels.numpy())\n",
    "                \n",
    "        accuracy = accuracy_score(y, y_hat)\n",
    "        \n",
    "        return accuracy, y_hat, y\n",
    "\n",
    "# hyperparameters\n",
    "# - batch_size set above in previous cell\n",
    "\n",
    "input_size = 256\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "num_epochs = 500\n",
    "loss_kind = nn.BCELoss() # binary cross entropy\n",
    "\n",
    "LSTM_RNN = LSTM_Model(input_size, hidden_size, num_layers, output_size, batch_size)\n",
    "optimizer = Adam(LSTM_RNN.parameters(), lr = 0.005)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeccd98-f3ef-4be8-b0ca-c0aba2f5e8c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5. Train the above model. 30 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4602472-e077-4a69-b886-a618b19a8c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Training loss:  0.6888541579246521\n",
      "Validation loss:  0.6695873737335205\n",
      "Epoch:  2\n",
      "Training loss:  0.6953457593917847\n",
      "Validation loss:  0.6957165598869324\n",
      "Epoch:  3\n",
      "Training loss:  0.6865209341049194\n",
      "Validation loss:  0.6920449733734131\n",
      "Epoch:  4\n",
      "Training loss:  0.6923566460609436\n",
      "Validation loss:  0.6977317333221436\n",
      "Epoch:  5\n",
      "Training loss:  0.6908106207847595\n",
      "Validation loss:  0.6979081630706787\n",
      "Epoch:  6\n",
      "Training loss:  0.6935461163520813\n",
      "Validation loss:  0.7046772241592407\n",
      "Epoch:  7\n",
      "Training loss:  0.6977272033691406\n",
      "Validation loss:  0.6945155262947083\n",
      "Epoch:  8\n",
      "Training loss:  0.6883686780929565\n",
      "Validation loss:  0.6987269520759583\n",
      "Epoch:  9\n",
      "Training loss:  0.7035752534866333\n",
      "Validation loss:  0.6878331303596497\n",
      "Epoch:  10\n",
      "Training loss:  0.6949731707572937\n",
      "Validation loss:  0.6879757642745972\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train RNN model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 5\u001b[0m train_loss, validation_loss \u001b[38;5;241m=\u001b[39m \u001b[43mLSTM_RNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mloss_kind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_kind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# print train time\u001b[39;00m\n\u001b[0;32m     12\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m ((time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 38\u001b[0m, in \u001b[0;36mLSTM_Model.train_model\u001b[1;34m(self, train_dataloader, validation_dataloader, num_epochs, loss_kind, optimizer)\u001b[0m\n\u001b[0;32m     36\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     39\u001b[0m tr_loss \u001b[38;5;241m=\u001b[39m loss_kind(outputs, labels)\n\u001b[0;32m     40\u001b[0m tr_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mLSTM_Model.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#h0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size) #  X.shape[1] or 0 or batch_size) !!!!!!!!\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#c0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size) #  X.shape[1] or ) !!!!!!!!\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#out, _ = self.lstm(X, (h0, c0))\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#X = self.embedding = torch.nn.Embedding(input_size, 256)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     out, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#out = out[:, -1, :]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#out = out[:, :]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:911\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    908\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 911\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    915\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train RNN model\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_loss, validation_loss = LSTM_RNN.train_model(train_dataloader = train_dataloader,\n",
    "                                                       validation_dataloader = validation_dataloader,\n",
    "                                                       num_epochs = num_epochs,\n",
    "                                                       loss_kind = loss_kind,\n",
    "                                                       optimizer = optimizer)\n",
    "\n",
    "# print train time\n",
    "elapsed_time = ((time.time() - start_time) / 60)\n",
    "print(\"Elapsed model training time:\\n{:.2f} minutes\".format(elapsed_time))\n",
    "\n",
    "# plot loss curves\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), train_loss, label = 'Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), validation_loss, label = 'Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5de83-c9c6-4383-aa34-de0f5299360a",
   "metadata": {},
   "source": [
    "#### 6. Calculate and report the performance of your model on the training and test set. 15 points\n",
    "- Calculate the accuracies on the training and test sets\n",
    "- Print out the confusion matrix of model results on the test set: https://scikit-learn.org/dev/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ac7b3-a5d7-4214-9ca4-897c3d54cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "\n",
    "train_accuracy, y_train_hat, y_train = LSTM_RNN.evaluate(train_dataloader)\n",
    "test_accuracy, y_test_hat, y_test = LSTM_RNN.evaluate(train_dataloader)\n",
    "\n",
    "# print accuracies\n",
    "\n",
    "print(\"Training accuracy:\\n\", train_accuracy)\n",
    "print()\n",
    "print(\"Test accuracy:\\n\", test_accuracy)\n",
    "print()\n",
    "\n",
    "# confusion matrices\n",
    "\n",
    "print(\"Confusion matrix for train set:\\n\", confusion_matrix(y_train, y_train_hat))\n",
    "print()\n",
    "print(\"Confusion matrix for test set:\\n\", confusion_matrix(y_test, y_test_hat))\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
